{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "network-compression-using-SVD.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "dApr2_DR9tXi",
        "G369tl4rIY_t",
        "r13HwBP_Iilt",
        "HZ6nWUeSIvDw",
        "XKk1SvfaI5yu",
        "v4GaEZCHJGVe",
        "hbJ45xE6JbHv"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItH59YVg9rBD",
        "colab_type": "text"
      },
      "source": [
        "# Project summary:\n",
        "\n",
        "* Implement a Dense NN with 5 hidden layers for MNIST digit classification\n",
        "* Compress the weight matrices using Singular Value Decomposition (SVD)\n",
        "* Improve the accuracy of the compressed network by carrying out low rank approximation of weights during network training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6RmCS6Yehll",
        "colab_type": "text"
      },
      "source": [
        "### Models:\n",
        "\n",
        "The first model, henceforth referred to as the **Teacher Network**, has five hidden **FC** layers of $1024$ units each followed by the last layer with $10$ units. All the layers are activated using **ReLU** non-linearity, except for the last layer that uses **Softmax** to predict the output class. Similar to the previous assignments, the model tries to minimize the **Softmax cross-entropy** loss using the **Adam** optimizer with learning rate set to $10^{-3}$ and converges at a loss of $0.2126$ over just $31$ epochs for a batch size of 50 samples.\n",
        "\n",
        "The second model, aka the **Student Network**, uses the low-rank approximated weights of the Teacher network as its starting point. Needless to say, the overall structure of the Student network, including the number of layers, units and the activation functions, are exactly same as that of the Teacher network. The training of the Student network is done by, yet again, carrying out the low-rank approximation of weights at each layer to compute $\\hat{W}$ and feeding them forward to the next layer (except for the last layer). However, during backpropagation, a custom gradient function is used to assign the differentiation of this approximation as $1$, thereby updating $W$ rather than $\\hat{W}$. This network converges at a loss of $0.2213$ over $30$ epcohs for a batch size of $100$ samples.\n",
        "\n",
        "### Output:\n",
        "\n",
        "The Teacher network achieves train and test accuracies north of $99\\%$ and $98\\%$ respectively. The Student network, which had a baseline accuracy of $91.81\\%$, converges at $96\\%$ training and testing accuracies.\n",
        "\n",
        "### Narrative:\n",
        "The only tricky part in this assignment was figuring out how to create a custom gradient function. Initially, I tried running the Student network without specifying the gradient function, and it worked fine giving an accuracy of $96\\%$. After researching a bit online, I found that TensorFlow automatically assumes the custom gradient to be unity in case we have not explicitly specified the gradient.\n",
        "\n",
        "Having said that, after reading up the TF documentation (which is really poor, by the way) and looking at similar examples on the web, I was able to implement my own custom gradient function. The trick was to use the `tf@RegisterGradient` decorator to define the custom gradient function while building the graph, and then using the `gradient_override_map` method on the graph that mapped the gradient of the **Identity** function to the gradient function registered in the graph. Obviously, I had to wrap the outputs of all layers (calculated using the approximated weights) in `tf.identity` function withing the override method so that the mapping is triggered at the time of execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dApr2_DR9tXi",
        "colab_type": "text"
      },
      "source": [
        "## Boot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q90IVx1Yd2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "%load_ext tensorboard\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# !pip install librosa\n",
        "import librosa\n",
        "import IPython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu4jDG5xWkLM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b325686c-d77f-453a-c1c2-3905ce3b721d"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsfqwekS9vkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9f932fdf-6171-46b6-ccaf-80f5b011d19e"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()\n",
        "\n",
        "!cat /proc/cpuinfo\n",
        "!cat /proc/meminfo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2300.000\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\n",
            "bogomips\t: 4600.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2300.000\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs\n",
            "bogomips\t: 4600.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "MemTotal:       13335192 kB\n",
            "MemFree:        10120300 kB\n",
            "MemAvailable:   12278368 kB\n",
            "Buffers:           73564 kB\n",
            "Cached:          2224328 kB\n",
            "SwapCached:            0 kB\n",
            "Active:           873776 kB\n",
            "Inactive:        1989260 kB\n",
            "Active(anon):     527172 kB\n",
            "Inactive(anon):     2376 kB\n",
            "Active(file):     346604 kB\n",
            "Inactive(file):  1986884 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:               392 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        565152 kB\n",
            "Mapped:           474196 kB\n",
            "Shmem:              2948 kB\n",
            "Slab:             169984 kB\n",
            "SReclaimable:     131112 kB\n",
            "SUnreclaim:        38872 kB\n",
            "KernelStack:        4608 kB\n",
            "PageTables:         7800 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6667596 kB\n",
            "Committed_AS:    3492908 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:           0 kB\n",
            "VmallocChunk:          0 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "DirectMap4k:      149492 kB\n",
            "DirectMap2M:     5093376 kB\n",
            "DirectMap1G:    10485760 kB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G369tl4rIY_t",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7jN4DvjZGz0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "efd80c01-f293-4f39-a01c-4a35ade0ed40"
      },
      "source": [
        "# Load MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-93d8da72a918>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r13HwBP_Iilt",
        "colab_type": "text"
      },
      "source": [
        "## Build graph for Teacher network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MvbyL-PcuiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "  Teacher Neural Network\n",
        "'''\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "!rm -rf ./logs/\n",
        "STORE_PATH = \"./logs/\"\n",
        "\n",
        "# initialize tensors\n",
        "x = tf.placeholder(tf.float32, shape=(784, None), name=\"inputs\")\n",
        "y = tf.placeholder(tf.float32, shape=(10, None), name=\"labels\")\n",
        "lr = tf.constant(0.0001)\n",
        "init = tf.initializers.glorot_normal()\n",
        "\n",
        "def create_tensors(m, n, H, w_name, b_name):\n",
        "  W = tf.Variable(init([m, n], dtype=tf.float32), dtype=tf.float32, name=w_name)\n",
        "  b = tf.Variable(init([n, 1], dtype=tf.float32), dtype=tf.float32, name=b_name)\n",
        "  if n == 10:\n",
        "    Y = tf.nn.softmax(tf.transpose(W)@H + b)\n",
        "  else:\n",
        "    Y = tf.nn.relu(tf.transpose(W, name=\"trans\"+w_name[-3:])@H + b, name=\"relu\"+w_name[-3:])\n",
        "  return W, b, Y\n",
        "\n",
        "# implement 5 fully-connected hidden layers\n",
        "Wi, bi, Yi = create_tensors(784, 1024, x, \"wt_Li\", \"bias_Li\")\n",
        "W1, b1, Y1 = create_tensors(1024, 1024, Yi, \"wt_L1\", \"bias_L1\")\n",
        "W2, b2, Y2 = create_tensors(1024, 1024, Y1, \"wt_L2\", \"bias_L2\")\n",
        "W3, b3, Y3 = create_tensors(1024, 1024, Y2, \"wt_L3\", \"bias_L3\")\n",
        "W4, b4, Y4 = create_tensors(1024, 1024, Y3, \"wt_L4\", \"bias_L4\")\n",
        "Wo, bo, Yo = create_tensors(1024, 10, Y4, \"wt_Lo\", \"bias_Lo\")\n",
        "\n",
        "# loss and optimizer\n",
        "loss = tf.reduce_mean(-y*tf.log(tf.exp(Yo)/tf.reduce_sum(tf.exp(Yo), 0, True)), name=\"loss\")\n",
        "opt = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
        "\n",
        "# compute accuracy\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, axis=0), tf.argmax(Yo, axis=0)), dtype=tf.float32), name=\"accuracy\")\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    writer = tf.summary.FileWriter(STORE_PATH, sess.graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ6nWUeSIvDw",
        "colab_type": "text"
      },
      "source": [
        "## Fit model on Teacher network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqULblj__q7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34SBTQdnXR-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "975853d8-8d61-4777-a8fa-d14df8da84fb"
      },
      "source": [
        "# Fit and evaluate\n",
        "dim = mnist.train.num_examples\n",
        "b_size = 50\n",
        "maxIter = 40\n",
        "\n",
        "errt = np.zeros(maxIter)  # for storing the loss per epoch\n",
        "\n",
        "for i in range(maxIter):\n",
        "  epoch_loss = 0\n",
        "  for j in range(dim//b_size):\n",
        "    feat, lab = mnist.train.next_batch(b_size)\n",
        "    batch_loss, _ = sess.run([loss, opt], feed_dict={x: np.transpose(feat), y: np.transpose(lab)})\n",
        "    epoch_loss += batch_loss\n",
        "  # collect loss\n",
        "  errt[i] = epoch_loss * b_size / dim\n",
        "  # evaluate train and test accuracies\n",
        "  train_accu = sess.run(acc, feed_dict={x: np.transpose(mnist.train.images), y: np.transpose(mnist.train.labels)})\n",
        "  test_accu = sess.run(acc, feed_dict={x: np.transpose(mnist.test.images), y: np.transpose(mnist.test.labels)})\n",
        "  print(\"epoch: \", i+1, \"  loss: \", errt[i], \"  train acc: \", train_accu, \" test acc: \", test_accu)\n",
        "  # early stopping\n",
        "  if test_accu >= 0.98:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  1   loss:  0.21395367618311537   train acc:  0.8524  test acc:  0.8645\n",
            "epoch:  2   loss:  0.21309344024820762   train acc:  0.9238  test acc:  0.927\n",
            "epoch:  3   loss:  0.21294779968532648   train acc:  0.9399091  test acc:  0.9433\n",
            "epoch:  4   loss:  0.21285404059019955   train acc:  0.94274545  test acc:  0.9432\n",
            "epoch:  5   loss:  0.2128465810418129   train acc:  0.91785455  test acc:  0.9223\n",
            "epoch:  6   loss:  0.21280549243092536   train acc:  0.95967275  test acc:  0.9572\n",
            "epoch:  7   loss:  0.2127521503919905   train acc:  0.93363637  test acc:  0.9373\n",
            "epoch:  8   loss:  0.21274347630414095   train acc:  0.96689093  test acc:  0.965\n",
            "epoch:  9   loss:  0.21273233738812533   train acc:  0.9730545  test acc:  0.9699\n",
            "epoch:  10   loss:  0.21277180958877911   train acc:  0.9668546  test acc:  0.9619\n",
            "epoch:  11   loss:  0.21272344814105468   train acc:  0.9746909  test acc:  0.9697\n",
            "epoch:  12   loss:  0.21273506451736798   train acc:  0.9751818  test acc:  0.9682\n",
            "epoch:  13   loss:  0.21267692771824923   train acc:  0.9758545  test acc:  0.9695\n",
            "epoch:  14   loss:  0.21269766643643379   train acc:  0.9777273  test acc:  0.9698\n",
            "epoch:  15   loss:  0.2126949968663129   train acc:  0.9822  test acc:  0.9743\n",
            "epoch:  16   loss:  0.21268255205316977   train acc:  0.9839636  test acc:  0.9752\n",
            "epoch:  17   loss:  0.21263783838261258   train acc:  0.9806182  test acc:  0.9725\n",
            "epoch:  18   loss:  0.2126840202645822   train acc:  0.9837818  test acc:  0.9776\n",
            "epoch:  19   loss:  0.2126832681216977   train acc:  0.9854364  test acc:  0.9791\n",
            "epoch:  20   loss:  0.2127085053920746   train acc:  0.97932726  test acc:  0.9697\n",
            "epoch:  21   loss:  0.21267759057608518   train acc:  0.98538184  test acc:  0.9756\n",
            "epoch:  22   loss:  0.21265207466754046   train acc:  0.9857091  test acc:  0.9765\n",
            "epoch:  23   loss:  0.21266230323097923   train acc:  0.9802909  test acc:  0.9708\n",
            "epoch:  24   loss:  0.21263406157493592   train acc:  0.98398185  test acc:  0.9734\n",
            "epoch:  25   loss:  0.21265473957766187   train acc:  0.9861091  test acc:  0.9757\n",
            "epoch:  26   loss:  0.21267289247025142   train acc:  0.9816  test acc:  0.9702\n",
            "epoch:  27   loss:  0.21265333586118437   train acc:  0.9883818  test acc:  0.9756\n",
            "epoch:  28   loss:  0.21264228562062437   train acc:  0.9919636  test acc:  0.9798\n",
            "epoch:  29   loss:  0.2126512109285051   train acc:  0.9897636  test acc:  0.9778\n",
            "epoch:  30   loss:  0.21263843469999053   train acc:  0.9894364  test acc:  0.9775\n",
            "epoch:  31   loss:  0.21263402594761416   train acc:  0.9911091  test acc:  0.9812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKk1SvfaI5yu",
        "colab_type": "text"
      },
      "source": [
        "## Compress trained weights and evaluate baseline results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xCpIrgMYsoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the learned weights\n",
        "w_L1, b_L1, w_L2, b_L2, w_L3, b_L3, w_L4, b_L4, w_L5, b_L5 = sess.run([Wi, bi, W1, b1, W2, b2, W3, b3, W4, b4])\n",
        "\n",
        "# Carry out SVD on weights\n",
        "s_L1, u_L1, v_L1 = tf.linalg.svd(w_L1)\n",
        "s_L2, u_L2, v_L2 = tf.linalg.svd(w_L2)\n",
        "s_L3, u_L3, v_L3 = tf.linalg.svd(w_L3)\n",
        "s_L4, u_L4, v_L4 = tf.linalg.svd(w_L4)\n",
        "s_L5, u_L5, v_L5 = tf.linalg.svd(w_L5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3ZQpE8OH-0W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "302aa465-d594-4414-dea2-2ac6d5a2abd1"
      },
      "source": [
        "# Compute test accuracies for different compressions\n",
        "def set_wts(n, to_np=0):\n",
        "  w1 = tf.matmul(tf.matmul(u_L1[:,:D], tf.linalg.diag(s_L1[:D,])), tf.transpose(v_L1[:,:D]))\n",
        "  w2 = tf.matmul(tf.matmul(u_L2[:,:D], tf.linalg.diag(s_L2[:D,])), tf.transpose(v_L2[:,:D]))\n",
        "  w3 = tf.matmul(tf.matmul(u_L3[:,:D], tf.linalg.diag(s_L3[:D,])), tf.transpose(v_L3[:,:D]))\n",
        "  w4 = tf.matmul(tf.matmul(u_L4[:,:D], tf.linalg.diag(s_L4[:D,])), tf.transpose(v_L4[:,:D]))\n",
        "  w5 = tf.matmul(tf.matmul(u_L5[:,:D], tf.linalg.diag(s_L5[:D,])), tf.transpose(v_L5[:,:D]))\n",
        "  if to_np:\n",
        "    return w1.eval(), w2.eval(), w3.eval(), w4.eval(), w5.eval()\n",
        "  else:\n",
        "    return w1, w2, w3, w4, w5\n",
        "\n",
        "for D in [10, 20, 50, 100, 200, 784]:\n",
        "  w1, w2, w3, w4, w5 = set_wts(D)\n",
        "  sess.run(tf.assign(Wi, w1))\n",
        "  sess.run(tf.assign(W1, w2))\n",
        "  sess.run(tf.assign(W2, w3))\n",
        "  sess.run(tf.assign(W3, w4))\n",
        "  sess.run(tf.assign(W4, w5))\n",
        "  print(\"Evaluation results for D = \", D)\n",
        "  print(\"Test accuracy: \", sess.run(acc, feed_dict={x: np.transpose(mnist.test.images), y: np.transpose(mnist.test.labels)}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation results for D =  10\n",
            "Test accuracy:  0.7541\n",
            "Evaluation results for D =  20\n",
            "Test accuracy:  0.9181\n",
            "Evaluation results for D =  50\n",
            "Test accuracy:  0.9706\n",
            "Evaluation results for D =  100\n",
            "Test accuracy:  0.9733\n",
            "Evaluation results for D =  200\n",
            "Test accuracy:  0.9774\n",
            "Evaluation results for D =  784\n",
            "Test accuracy:  0.9813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4GaEZCHJGVe",
        "colab_type": "text"
      },
      "source": [
        "## Use compressed weights to build Student network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vFYG3j0gRFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract weights corr. to D=20 as numpy arrays\n",
        "w1, w2, w3, w4, w5 = set_wts(20, to_np=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vl65GJbIpzY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "ac1ead79-c50f-4bec-b08c-2992ca7f73e9"
      },
      "source": [
        "'''\n",
        "  Student Neural Network\n",
        "'''\n",
        "\n",
        "sess.close()\n",
        "tf.reset_default_graph()\n",
        "\n",
        "!rm -rf ./logs/\n",
        "STORE_PATH = \"./logs/\"\n",
        "\n",
        "# register custom gradient\n",
        "@tf.RegisterGradient(\"SvdGrad\")\n",
        "def svd_grad(op, grad):\n",
        "  return 1\n",
        "\n",
        "# function to carry out SVD operation\n",
        "def svd_op(w, b, H, w_name):\n",
        "  s, u, v = tf.linalg.svd(w, name=\"svd_op\"+w_name[-3:])\n",
        "  W_hat = tf.Variable(tf.matmul(tf.matmul(u[:,:20], tf.linalg.diag(s[:20,])), tf.transpose(v[:,:20])), name=w_name, trainable=False)\n",
        "  Y = tf.nn.relu(tf.transpose(W_hat, name=\"trans\"+w_name[-3:])@H + b, name=\"relu\"+w_name[-3:])\n",
        "  return Y\n",
        "\n",
        "# initialize tensors\n",
        "init = tf.initializers.glorot_normal()\n",
        "lr = tf.constant(0.001, name=\"lr\")\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=(784, None), name=\"inputs\")\n",
        "y = tf.placeholder(tf.float32, shape=(10, None), name=\"labels\")\n",
        "\n",
        "Wi = tf.Variable(w1, dtype=tf.float32, name=\"wt_Li\")\n",
        "W1 = tf.Variable(w2, dtype=tf.float32, name=\"wt_L1\")\n",
        "W2 = tf.Variable(w3, dtype=tf.float32, name=\"wt_L2\")\n",
        "W3 = tf.Variable(w4, dtype=tf.float32, name=\"wt_L3\")\n",
        "W4 = tf.Variable(w5, dtype=tf.float32, name=\"wt_L4\")\n",
        "Wo = tf.Variable(init([1024, 10], dtype=tf.float32), dtype=tf.float32, name=\"wt_Lo\")\n",
        "\n",
        "bi = tf.Variable(b_L1, dtype=tf.float32, name=\"bias_Li\")\n",
        "b1 = tf.Variable(b_L2, dtype=tf.float32, name=\"bias_L1\")\n",
        "b2 = tf.Variable(b_L3, dtype=tf.float32, name=\"bias_L2\")\n",
        "b3 = tf.Variable(b_L4, dtype=tf.float32, name=\"bias_L3\")\n",
        "b4 = tf.Variable(b_L5, dtype=tf.float32, name=\"bias_L4\")\n",
        "bo = tf.Variable(init([10, 1], dtype=tf.float32), dtype=tf.float32, name=\"bias_Lo\")\n",
        "\n",
        "Yi = svd_op(Wi, bi, x, \"W_hat_Li\")\n",
        "Y1 = svd_op(W1, b1, Yi, \"W_hat_L1\")\n",
        "Y2 = svd_op(W2, b2, Y1, \"W_hat_L2\")\n",
        "Y3 = svd_op(W3, b3, Y2, \"W_hat_L3\")\n",
        "Y4 = svd_op(W4, b4, Y3, \"W_hat_L4\")\n",
        "Yo = tf.nn.softmax(tf.transpose(Wo, name=\"trans_Lo\") @ Y4 + bo)\n",
        "\n",
        "g = tf.get_default_graph()\n",
        "\n",
        "# loss and optimizer\n",
        "loss = tf.reduce_mean(-y*tf.log(tf.exp(Yo)/tf.reduce_sum(tf.exp(Yo), 0, True)), name=\"loss\")\n",
        "opt = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
        "\n",
        "# compute accuracy\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, axis=0), tf.argmax(Yo, axis=0)), dtype=tf.float32), name=\"accuracy\")\n",
        "\n",
        "# override gradients\n",
        "with g.gradient_override_map({'Identity': 'SvdGrad'}):\n",
        "    Yi = tf.identity(Yi, name=\"svd_Li\")\n",
        "    Y1 = tf.identity(Y1, name=\"svd_L1\")\n",
        "    Y2 = tf.identity(Y2, name=\"svd_L2\")\n",
        "    Y3 = tf.identity(Y3, name=\"svd_L3\")\n",
        "    Y4 = tf.identity(Y4, name=\"svd_L4\")\n",
        "    grad_Li = tf.gradients(Yi, Wi, name=\"grad_Li\")\n",
        "    grad_L1 = tf.gradients(Y1, W1, name=\"grad_L1\")\n",
        "    grad_L2 = tf.gradients(Y2, W2, name=\"grad_L2\")\n",
        "    grad_L3 = tf.gradients(Y3, W3, name=\"grad_L3\")\n",
        "    grad_L4 = tf.gradients(Y4, W4, name=\"grad_L4\")\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    writer = tf.summary.FileWriter(STORE_PATH, sess.graph)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py:2825: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbJ45xE6JbHv",
        "colab_type": "text"
      },
      "source": [
        "## Fit and evaluate Student network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhC18Skd4lSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IHqw1Szz-1n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "317df772-2b77-4e31-c249-2778ce22de73"
      },
      "source": [
        "# Fit and evaluate\n",
        "dim = mnist.train.num_examples\n",
        "b_size = 100\n",
        "maxIter = 30\n",
        "\n",
        "errt = np.zeros(maxIter)  # for storing the loss per epoch\n",
        "\n",
        "for i in range(maxIter):\n",
        "  epoch_loss = 0\n",
        "  for j in range(dim//b_size):\n",
        "    feat, lab = mnist.train.next_batch(b_size)\n",
        "    batch_loss, _ = sess.run([loss, opt], feed_dict={x: np.transpose(feat), y: np.transpose(lab)})\n",
        "    epoch_loss += batch_loss\n",
        "  # collect loss\n",
        "  errt[i] = epoch_loss * b_size / dim\n",
        "  # evaluate train and test accuracies\n",
        "  train_accu = sess.run(acc, feed_dict={x: np.transpose(mnist.train.images), y: np.transpose(mnist.train.labels)})\n",
        "  test_accu = sess.run(acc, feed_dict={x: np.transpose(mnist.test.images), y: np.transpose(mnist.test.labels)})\n",
        "  print(\"epoch: \", i+1, \"  loss: \", errt[i], \"  train acc: \", train_accu, \" test acc: \", test_accu)\n",
        "  # early stopping\n",
        "  if test_accu >= 0.97:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  1   loss:  0.22152437635443428   train acc:  0.9528  test acc:  0.9531\n",
            "epoch:  2   loss:  0.22135312275453048   train acc:  0.9578182  test acc:  0.9582\n",
            "epoch:  3   loss:  0.2213447708975185   train acc:  0.96114546  test acc:  0.9597\n",
            "epoch:  4   loss:  0.22133866315538234   train acc:  0.9602182  test acc:  0.9591\n",
            "epoch:  5   loss:  0.2213400980017402   train acc:  0.96221817  test acc:  0.9616\n",
            "epoch:  6   loss:  0.22133784313093532   train acc:  0.96205455  test acc:  0.9602\n",
            "epoch:  7   loss:  0.22133790788325397   train acc:  0.96165454  test acc:  0.9607\n",
            "epoch:  8   loss:  0.22133393257856368   train acc:  0.964  test acc:  0.9635\n",
            "epoch:  9   loss:  0.2213344425775788   train acc:  0.9608727  test acc:  0.9618\n",
            "epoch:  10   loss:  0.22133299087936228   train acc:  0.96196365  test acc:  0.9619\n",
            "epoch:  11   loss:  0.22133188570087606   train acc:  0.9607818  test acc:  0.9606\n",
            "epoch:  12   loss:  0.22133179502053693   train acc:  0.9613091  test acc:  0.963\n",
            "epoch:  13   loss:  0.22133079480041157   train acc:  0.9631636  test acc:  0.9633\n",
            "epoch:  14   loss:  0.22132983180609617   train acc:  0.96238184  test acc:  0.9626\n",
            "epoch:  15   loss:  0.22132984055714175   train acc:  0.96401817  test acc:  0.9654\n",
            "epoch:  16   loss:  0.22132957325740293   train acc:  0.9625818  test acc:  0.9638\n",
            "epoch:  17   loss:  0.22132814060557973   train acc:  0.96361816  test acc:  0.964\n",
            "epoch:  18   loss:  0.2213280196894299   train acc:  0.9637455  test acc:  0.9641\n",
            "epoch:  19   loss:  0.22132772971283307   train acc:  0.96165454  test acc:  0.963\n",
            "epoch:  20   loss:  0.22132726094939492   train acc:  0.96085453  test acc:  0.9618\n",
            "epoch:  21   loss:  0.22133041194894096   train acc:  0.9602727  test acc:  0.9606\n",
            "epoch:  22   loss:  0.22132756934924558   train acc:  0.96181816  test acc:  0.9626\n",
            "epoch:  23   loss:  0.22132637958634985   train acc:  0.9635091  test acc:  0.9641\n",
            "epoch:  24   loss:  0.22132591353221373   train acc:  0.96327275  test acc:  0.9648\n",
            "epoch:  25   loss:  0.22132617527788337   train acc:  0.96205455  test acc:  0.9628\n",
            "epoch:  26   loss:  0.22132618140090596   train acc:  0.96425456  test acc:  0.9642\n",
            "epoch:  27   loss:  0.22133165196938948   train acc:  0.9645454  test acc:  0.9661\n",
            "epoch:  28   loss:  0.22132560488852587   train acc:  0.96345454  test acc:  0.9639\n",
            "epoch:  29   loss:  0.22132534872401843   train acc:  0.96361816  test acc:  0.9639\n",
            "epoch:  30   loss:  0.2213273297656666   train acc:  0.96256363  test acc:  0.9633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZcMJ_VV3pC1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "2a7a1c3a-a187-45c3-e69e-d2ca058c09ef"
      },
      "source": [
        "g.get_collection(\"trainable_variables\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'wt_Li:0' shape=(784, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'wt_L1:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'wt_L2:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'wt_L3:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'wt_L4:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'wt_Lo:0' shape=(1024, 10) dtype=float32_ref>,\n",
              " <tf.Variable 'bias_Li:0' shape=(1024, 1) dtype=float32_ref>,\n",
              " <tf.Variable 'bias_L1:0' shape=(1024, 1) dtype=float32_ref>,\n",
              " <tf.Variable 'bias_L2:0' shape=(1024, 1) dtype=float32_ref>,\n",
              " <tf.Variable 'bias_L3:0' shape=(1024, 1) dtype=float32_ref>,\n",
              " <tf.Variable 'bias_L4:0' shape=(1024, 1) dtype=float32_ref>,\n",
              " <tf.Variable 'bias_Lo:0' shape=(10, 1) dtype=float32_ref>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}